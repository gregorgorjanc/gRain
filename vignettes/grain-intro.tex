%\VignetteIndexEntry{grain-intro}
%\VignetteDepends{gRbase}
%\VignetteKeyword{Bayesian networks}
%\VignetteKeyword{Graphical Models}
%\VignetteKeyword{Probabilistic networks}
%\VignetteEngine{knitr::knitr} 

\documentclass[10pt]{article}\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{boxedminipage,color,a4wide,url}
\usepackage[utf8]{inputenc}

\usepackage[authoryear,round]{natbib}
\bibliographystyle{plainnat}



\usepackage{etoolbox} 
\makeatletter 
\preto{\@verbatim}{\topsep=0pt \partopsep=-25pt } 
\makeatother

\usepackage{alltt}
\AtBeginEnvironment{alltt}{\setlength{\topsep}{-25pt}}



\def\pkg#1{{\bf #1}}
\def\grbn{{\bf gRain}}
\def\grain{\texttt{grain}}
\def\code#1{{\texttt{#1}}}
\def\R{\texttt{R}}



\author{S{\o}ren H{\o}jsgaard\\Aalborg University, Denmark}
\title{Bayesian networks in R with the \pkg{gRain} package}
\date{\pkg{gRain} version 1.3-3 as of 2020-01-18}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\maketitle
\tableofcontents
\parindent0pt\parskip5pt

\section{Introduction}

The \grbn\ package implements Bayesian Networks (hereafter often
abbreviated BNs). The name \grbn\ is an acronym for [gra]phical
[i]ndependence [n]etworks. The main reference for \grbn\ to cite is
\cite{hoj:12}, see also

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{citation}\hlstd{(}\hlstr{"gRain"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## 
## To cite gRain in publications use:
## 
##   Søren Højsgaard (2012). Graphical Independence Networks with the gRain
##   Package for R. Journal of Statistical Software, 46(10), 1-26. URL
##   http://www.jstatsoft.org/v46/i10/.
## 
## A BibTeX entry for LaTeX users is
## 
##   @Article{,
##     title = {Graphical Independence Networks with the {gRain} Package for {R}},
##     author = {S{\o}ren H{\o}jsgaard},
##     journal = {Journal of Statistical Software},
##     year = {2012},
##     volume = {46},
##     number = {10},
##     pages = {1--26},
##     url = {http://www.jstatsoft.org/v46/i10/},
##   }
\end{verbatim}
\end{kframe}
\end{knitrout}

Moreover, \cite{hoj:edw:lau:12} gives a broad treatment of graphical
models (including Bayesian networks) More information about the
package, other graphical modelling packages and development versions
is available from

\begin{quote}
\url{http://people.math.aau.dk/~sorenh/software/gR}
\end{quote}

%% \section{Two worked examples}
%% \label{sec:two-worked-examples}

%% \subsection{Wet grass}
%% \label{sec:wet-grass}

%% The `wet grass` example is motivated by the following narrative (taken from 
%% \url{https://en.wikipedia.org/wiki/Bayesian_network})
%% \begin{quote}
%%   Two events can cause grass to be wet: an active sprinkler or
%%   rain. Rain has a direct effect on the use of the sprinkler (namely
%%   that when it rains, the sprinkler usually is not active).
%% \end{quote}

%% <<echo=F, results='hide'>>=
%% p.R    <- cptable(~R, values=c(2, 8), levels=yn)
%% p.S_R  <- cptable(~S:R, values=c(1, 99, 4, 6), levels=yn)
%% p.G_SR <- cptable(~G:S:R, values=c(99, 1, 8, 2, 9, 1, 0, 1), levels=yn)
%% grass_bn <- compileCPT(p.R, p.S_R, p.G_SR)  %>% grain
%% @ 

%% <<chest-grass, fig.height=2, echo=F, fig.cap="Wet graph example; taken from Wikipedia.">>=
%% plot(grass_bn)
%% @ %def


\section{Example: Chest clinic}
\label{sec:chest-clinic}
\label{sec:chest}



This section reviews the chest clinic example of \cite{lau/spieg:88}
(illustrated in Figure~\ref{fig:chest-LS}) and shows one way of
specifying the model in \grbn{}.  \cite{lau/spieg:88} motivate the
chest clinic example with the following narrative:

\begin{quote}
  ``Shortness--of--breath (dyspnoea) may be due to tuberculosis, lung
  cancer or bronchitis, or none of them, or more than one of them. A
  recent visit to Asia increases the chances of tuberculosis, while
  smoking is known to be a risk factor for both lung cancer and
  bronchitis. The results of a single chest X--ray do not discriminate
  between lung cancer and tuberculosis, as neither does the presence or
  absence of dyspnoea.''
\end{quote}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{figure}
\includegraphics[width=\maxwidth]{figures/grain-chest-LS-1} \caption[Chest clinic example from Lauritzen and Spiegelhalter (1988)]{Chest clinic example from Lauritzen and Spiegelhalter (1988).}\label{fig:chest-LS}
\end{figure}


\end{knitrout}

\subsection{Building a network}

The description above involves the following binary variables:
$\alpha=\mbox{asia}$,
$\sigma=\mbox{smoker}$,
$\tau=\mbox{tuberculosis}$,
$\lambda=\mbox{lung cancer}$,
$\beta=\mbox{bronchitis}$,
$\epsilon=\mbox{either tuberculosis or lung cancer}$,
$\delta=\mbox{dyspnoea}$ and
$\xi=\mbox{xray}$. 
Each variable is binary and can take the values ``yes'' and ``no'':
Note that $\epsilon$ is a logical variable which is
true (yes) if either $\tau$ or $\lambda$ are true (yes) and false (no) otherwise.
The connection between the variables is displayed by the DAG (directed acyclic graph) in
Figure~\ref{fig:chest-LS}.

A joint probability density factorising accoring to a DAG with nodes
$V$ can be constructed as follows: Each node $v\in V$ has a set $pa(v)$ of parents and each node
$v\in V$ has a finite set of states. A joint distribution
over the variables $V$ can be given as
\begin{equation}
  \label{eq:dagfact1}
  p(V) = \prod_{v\in V} p(v|pa(v))
\end{equation}
where $p(v|pa(v))$ is a function defined on $(v,pa(v))$. This function
satisfies that $\sum_{v^*} p(v=v^*|pa(v))=1$, i.e.\ that
for each configuration of the parents $pa(v)$, the sum
over the levels of $v$ equals one. Hence $p(v|pa(v))$ becomes the
conditional distribution of $v$ given $pa(v)$.
In practice $p(v|pa(v))$ is specified as a table called a conditional
probability table or a CPT for short.
Thus, a Bayesian network can be regarded as a complex stochastic model built up by
putting together simple components (conditional probability
distributions).
A joint probability density for all eight variables in
Figure~\ref{fig:chest-LS}
can be constructed as 
\begin{equation}
  \label{eq:chestfact1}
  p(V) =
  p(\alpha)p(\sigma)p(\tau|\alpha)p(\lambda|\sigma)p(\beta|\sigma)p(\epsilon|\tau,\lambda)
  p(\delta|\epsilon, \beta)p(\xi|\epsilon).
\end{equation}



\subsection{Queries to networks}
\label{sec:xxx}

Suppose we are given the evidence (sometimes also called ``finding'')
that a set of variables $E\subset V$
have a specific value $e^*$.
With this evidence, we are often interested in the conditional
distribution $p(v|E=e^*)$
for some of the variables $v \in V \setminus E$
or in $p(U|E=e^*)$
for a set $U\subset V \setminus E$. Interest might also be in
calculating the probability of a specific event, e.g.\ the probability
of seeing a specific evidence, i.e.\ $p(E=e^*)$.
Other types of evidence (called soft evidence, virtual evidence or likelihood evidence) are discussed in
Section~\ref{sec:hard-virt-likel}.

For
example that a person has recently visited Asia and suffers from
dyspnoea, i.e.\ $\alpha=\mbox{yes}$ and $\delta=\mbox{yes}$.
In the chest clinic example, interest might be in $p(\lambda|e^*)$, $p(\tau|e^*)$
and  $p(\beta|e^*)$, or possibly in the joint (conditional) distribution
$p(\lambda,\tau,\beta|e^*)$.


\section{A one--minute version of  \grbn{}}
\label{sec:oneminute}

\subsection{Specifying a network}
\label{sec:specifying-network}

A simple way of  specifying the model for the chest clinic
example is as follows.

\begin{enumerate}
\item Specify conditional probability tables (with values as given in
  \cite{lau/spieg:88}) (there are other ways of specifying conditional
  probability tables, see the package documentation):

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{yn} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"yes"}\hlstd{,} \hlstr{"no"}\hlstd{)}
\hlstd{a}    \hlkwb{<-} \hlkwd{cptable}\hlstd{(}\hlopt{~}\hlstd{asia,} \hlkwc{values}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{99}\hlstd{),} \hlkwc{levels}\hlstd{=yn)}
\hlstd{t.a}  \hlkwb{<-} \hlkwd{cptable}\hlstd{(}\hlopt{~}\hlstd{tub}\hlopt{|}\hlstd{asia,} \hlkwc{values}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{5}\hlstd{,} \hlnum{95}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{99}\hlstd{),} \hlkwc{levels}\hlstd{=yn)}
\hlstd{s}    \hlkwb{<-} \hlkwd{cptable}\hlstd{(}\hlopt{~}\hlstd{smoke,} \hlkwc{values}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{5}\hlstd{,} \hlnum{5}\hlstd{),} \hlkwc{levels}\hlstd{=yn)}
\hlstd{l.s}  \hlkwb{<-} \hlkwd{cptable}\hlstd{(}\hlopt{~}\hlstd{lung}\hlopt{|}\hlstd{smoke,} \hlkwc{values}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{9}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{99}\hlstd{),} \hlkwc{levels}\hlstd{=yn)}
\hlstd{b.s}  \hlkwb{<-} \hlkwd{cptable}\hlstd{(}\hlopt{~}\hlstd{bronc}\hlopt{|}\hlstd{smoke,} \hlkwc{values}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{6}\hlstd{,} \hlnum{4}\hlstd{,} \hlnum{3}\hlstd{,} \hlnum{7}\hlstd{),} \hlkwc{levels}\hlstd{=yn)}
\hlstd{e.lt} \hlkwb{<-} \hlkwd{cptable}\hlstd{(}\hlopt{~}\hlstd{either}\hlopt{|}\hlstd{lung}\hlopt{:}\hlstd{tub,} \hlkwc{values}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{1}\hlstd{),} \hlkwc{levels}\hlstd{=yn)}
\hlstd{x.e}  \hlkwb{<-} \hlkwd{cptable}\hlstd{(}\hlopt{~}\hlstd{xray}\hlopt{|}\hlstd{either,} \hlkwc{values}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{98}\hlstd{,} \hlnum{2}\hlstd{,} \hlnum{5}\hlstd{,} \hlnum{95}\hlstd{),} \hlkwc{levels}\hlstd{=yn)}
\hlstd{d.be} \hlkwb{<-} \hlkwd{cptable}\hlstd{(}\hlopt{~}\hlstd{dysp}\hlopt{|}\hlstd{bronc}\hlopt{:}\hlstd{either,} \hlkwc{values}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{9}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{7}\hlstd{,} \hlnum{3}\hlstd{,} \hlnum{8}\hlstd{,} \hlnum{2}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{9}\hlstd{),} \hlkwc{levels}\hlstd{=yn)}
\end{alltt}
\end{kframe}
\end{knitrout}

\item Compile list of conditional probability tables.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{chest_cpt} \hlkwb{<-} \hlkwd{compileCPT}\hlstd{(a, t.a, s, l.s, b.s, e.lt, x.e, d.be)}
\hlkwd{summary}\hlstd{(chest_cpt)}
\end{alltt}
\begin{verbatim}
## cpt_spec with probabilities:
##  P( asia )
##  P( tub | asia )
##  P( smoke )
##  P( lung | smoke )
##  P( bronc | smoke )
##  P( either | lung tub )
##  P( xray | either )
##  P( dysp | bronc either )
\end{verbatim}
\end{kframe}
\end{knitrout}

The components are arrays, but coercion into dataframes sometimes makes it easier to digest the components.
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{chest_cpt}\hlopt{$}\hlstd{tub}
\end{alltt}
\begin{verbatim}
##      asia
## tub    yes   no
##   yes 0.05 0.01
##   no  0.95 0.99
\end{verbatim}
\begin{alltt}
\hlstd{chest_cpt}\hlopt{$}\hlstd{tub}  \hlopt{%>%} \hlstd{as.data.frame.table}
\end{alltt}
\begin{verbatim}
##   tub asia Freq
## 1 yes  yes 0.05
## 2  no  yes 0.95
## 3 yes   no 0.01
## 4  no   no 0.99
\end{verbatim}
\end{kframe}
\end{knitrout}

Notice: \code{either} is a logical node
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{chest_cpt}\hlopt{$}\hlstd{either}  \hlopt{%>%} \hlstd{as.data.frame.table}
\end{alltt}
\begin{verbatim}
##   either lung tub Freq
## 1    yes  yes yes    1
## 2     no  yes yes    0
## 3    yes   no yes    1
## 4     no   no yes    0
## 5    yes  yes  no    1
## 6     no  yes  no    0
## 7    yes   no  no    0
## 8     no   no  no    1
\end{verbatim}
\end{kframe}
\end{knitrout}


\item Create the network:\footnote{SH: Rethink print method}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{chest_bn} \hlkwb{<-} \hlkwd{grain}\hlstd{(chest_cpt)}
\hlstd{chest_bn}
\end{alltt}
\begin{verbatim}
## Independence network: Compiled: FALSE Propagated: FALSE 
##   Nodes: chr [1:8] "asia" "tub" "smoke" "lung" "bronc" "either" "xray" "dysp"
\end{verbatim}
\end{kframe}
\end{knitrout}

Compile the network (see references for details about
this):\footnote{SH: Maybe change so that default is that a network is
  compiled on creation time.}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{chest_bn} \hlkwb{<-} \hlkwd{compile}\hlstd{(chest_bn)}
\end{alltt}
\end{kframe}
\end{knitrout}

\end{enumerate}

\subsection{Querying a network}
\label{sec:querying-network}

\begin{enumerate}
\item The network can be queried to give marginal
  probabilities:\footnote{\code{querygrain()} can be abbreviated
    \code{qgrain()}.}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{querygrain}\hlstd{(chest_bn,} \hlkwc{nodes}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{"lung"}\hlstd{,} \hlstr{"bronc"}\hlstd{),} \hlkwc{type}\hlstd{=}\hlstr{"marginal"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## $lung
## lung
##   yes    no 
## 0.055 0.945 
## 
## $bronc
## bronc
##  yes   no 
## 0.45 0.55
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{querygrain}\hlstd{(chest_bn,} \hlkwc{nodes}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{"lung"}\hlstd{,} \hlstr{"bronc"}\hlstd{),} \hlkwc{type}\hlstd{=}\hlstr{"marginal"}\hlstd{)}  \hlopt{%>%} \hlkwd{lapply}\hlstd{(as.data.frame.table)}
\end{alltt}
\begin{verbatim}
## $lung
##   lung  Freq
## 1  yes 0.055
## 2   no 0.945
## 
## $bronc
##   bronc Freq
## 1   yes 0.45
## 2    no 0.55
\end{verbatim}
\end{kframe}
\end{knitrout}

\item Likewise, a joint distribution can be obtained:\footnote{SH: FIXME}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{querygrain}\hlstd{(chest_bn,} \hlkwc{nodes}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{"lung"}\hlstd{,} \hlstr{"bronc"}\hlstd{),} \hlkwc{type}\hlstd{=}\hlstr{"joint"}\hlstd{)}
\end{alltt}
\begin{verbatim}
##      bronc
## lung     yes     no
##   yes 0.0315 0.0235
##   no  0.4185 0.5265
\end{verbatim}
\end{kframe}
\end{knitrout}

\item Evidence can be entered in one of these two equivalent forms:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{chest_bn2}  \hlkwb{<-} \hlkwd{setEvidence}\hlstd{(chest_bn,} \hlkwc{evidence}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{asia}\hlstd{=}\hlstr{"yes"}\hlstd{,} \hlkwc{dysp}\hlstd{=}\hlstr{"yes"}\hlstd{))}
\hlstd{chest_bn2}  \hlkwb{<-} \hlkwd{setEvidence}\hlstd{(chest_bn,}
                      \hlkwc{nodes}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{"asia"}\hlstd{,} \hlstr{"dysp"}\hlstd{),} \hlkwc{states}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{"yes"}\hlstd{,} \hlstr{"yes"}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}

\item The probability of observing this evidence under the model is
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{pEvidence}\hlstd{(chest_bn2)}
\end{alltt}
\begin{verbatim}
## [1] 0.004501375
\end{verbatim}
\end{kframe}
\end{knitrout}

\item The network can be queried again:\footnote{SH: FIXME}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{querygrain}\hlstd{(chest_bn2,} \hlkwc{nodes}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{"lung"}\hlstd{,} \hlstr{"bronc"}\hlstd{))}
\end{alltt}
\begin{verbatim}
## $lung
## lung
##        yes         no 
## 0.09952515 0.90047485 
## 
## $bronc
## bronc
##       yes        no 
## 0.8114021 0.1885979
\end{verbatim}
\begin{alltt}
\hlkwd{querygrain}\hlstd{(chest_bn2,} \hlkwc{nodes}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{"lung"}\hlstd{,} \hlstr{"bronc"}\hlstd{),} \hlkwc{type}\hlstd{=}\hlstr{"joint"}\hlstd{)}
\end{alltt}
\begin{verbatim}
##      bronc
## lung          yes         no
##   yes 0.000283500 0.00016450
##   no  0.003368925 0.00068445
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{enumerate}


Notice a small shortcut: A common usage of a Bayesian network is to
enter evidence and then ask for the conditional distribtion of some
variables: This can be accomplished in one simple step as follows:\footnote{SH: FIXME}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{querygrain}\hlstd{(chest_bn,} \hlkwc{evidence}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{asia}\hlstd{=}\hlstr{"yes"}\hlstd{,} \hlkwc{dysp}\hlstd{=}\hlstr{"yes"}\hlstd{),}
           \hlkwc{nodes}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{"lung"}\hlstd{,} \hlstr{"bronc"}\hlstd{),} \hlkwc{type}\hlstd{=}\hlstr{"joint"}\hlstd{)}
\end{alltt}
\begin{verbatim}
##      bronc
## lung          yes         no
##   yes 0.000283500 0.00016450
##   no  0.003368925 0.00068445
\end{verbatim}
\end{kframe}
\end{knitrout}



\subsection{Conditioning on evidence with zero probability}
\label{sec:zero-probabilities}

Consider setting the evidence
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{chest_bn3} \hlkwb{<-} \hlkwd{setEvidence}\hlstd{(chest_bn,} \hlkwc{evidence}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{either}\hlstd{=}\hlstr{"no"}\hlstd{,} \hlkwc{tub}\hlstd{=}\hlstr{"yes"}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}

Under the model, this specific evidence has zero probability:
\verb|either| is true if \verb|tub| is true or \verb|lung| is true (or
both). Hence the specific evidence is impossible and therefore, all
conditional probabilities are (under the model) undefined:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{pEvidence}\hlstd{(chest_bn3)}
\end{alltt}
\begin{verbatim}
## [1] 0
\end{verbatim}
\begin{alltt}
\hlkwd{querygrain}\hlstd{(chest_bn3,} \hlkwc{nodes}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{"lung"}\hlstd{,} \hlstr{"bronc"}\hlstd{),} \hlkwc{type}\hlstd{=}\hlstr{"joint"}\hlstd{)}
\end{alltt}
\begin{verbatim}
##      bronc
## lung  yes no
##   yes   0  0
##   no    0  0
\end{verbatim}
\end{kframe}
\end{knitrout}



Zero probailities (or almost zero probabilities) also arise in a
different in a different setting. Consider this example

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{yn} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"yes"}\hlstd{,}\hlstr{"no"}\hlstd{)}
\hlstd{eps} \hlkwb{<-} \hlnum{1e-100}
\hlstd{a}    \hlkwb{<-} \hlkwd{cptable}\hlstd{(}\hlopt{~}\hlstd{a,}   \hlkwc{values}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{, eps),} \hlkwc{levels}\hlstd{=yn)}
\hlstd{b.a}  \hlkwb{<-} \hlkwd{cptable}\hlstd{(}\hlopt{~}\hlstd{b}\hlopt{+}\hlstd{a,} \hlkwc{values}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{, eps, eps,} \hlnum{1}\hlstd{),} \hlkwc{levels}\hlstd{=yn)}
\hlstd{c.b}  \hlkwb{<-} \hlkwd{cptable}\hlstd{(}\hlopt{~}\hlstd{c}\hlopt{+}\hlstd{b,} \hlkwc{values}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{, eps, eps,} \hlnum{1}\hlstd{),} \hlkwc{levels}\hlstd{=yn)}
\hlstd{plist} \hlkwb{<-} \hlkwd{compileCPT}\hlstd{(}\hlkwd{list}\hlstd{(a, b.a, c.b))}
\hlstd{bn}   \hlkwb{<-} \hlkwd{grain}\hlstd{(plist)}
\hlstd{tt}   \hlkwb{<-} \hlkwd{querygrain}\hlstd{(bn,} \hlkwc{type}\hlstd{=}\hlstr{"joint"}\hlstd{)}
\hlkwd{ftable}\hlstd{(tt)}
\end{alltt}
\begin{verbatim}
##         c    yes     no
## a   b                  
## yes yes    1e+00 1e-100
##     no    1e-200 1e-100
## no  yes   1e-200 1e-300
##     no    1e-200 1e-100
\end{verbatim}
\begin{alltt}
\hlkwd{querygrain}\hlstd{(}\hlkwd{setEvidence}\hlstd{(bn,} \hlkwc{evidence}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{a}\hlstd{=}\hlstr{"no"}\hlstd{,} \hlkwc{c}\hlstd{=}\hlstr{"yes"}\hlstd{)))}
\end{alltt}
\begin{verbatim}
## $b
## b
## yes  no 
## 0.5 0.5
\end{verbatim}
\end{kframe}
\end{knitrout}

No problem so far, but if \code{eps} is made smaller numerical
problems arise:\footnote{Her vil det netop vaere smart at laegge tabeller ind separat!!}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{eps}  \hlkwb{<-} \hlnum{1e-200}
\hlstd{a}    \hlkwb{<-} \hlkwd{cptable}\hlstd{(}\hlopt{~}\hlstd{a,}   \hlkwc{values}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{, eps),}\hlkwc{levels}\hlstd{=yn)}
\hlstd{b.a}  \hlkwb{<-} \hlkwd{cptable}\hlstd{(}\hlopt{~}\hlstd{b}\hlopt{+}\hlstd{a,} \hlkwc{values}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{, eps, eps,} \hlnum{1}\hlstd{),}\hlkwc{levels}\hlstd{=yn)}
\hlstd{c.b}  \hlkwb{<-} \hlkwd{cptable}\hlstd{(}\hlopt{~}\hlstd{c}\hlopt{+}\hlstd{b,} \hlkwc{values}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{, eps, eps,} \hlnum{1}\hlstd{),}\hlkwc{levels}\hlstd{=yn)}
\hlstd{plist} \hlkwb{<-} \hlkwd{compileCPT}\hlstd{(}\hlkwd{list}\hlstd{(a, b.a, c.b))}
\hlstd{bn}   \hlkwb{<-} \hlkwd{grain}\hlstd{(plist)}
\hlstd{tt}   \hlkwb{<-} \hlkwd{querygrain}\hlstd{(bn,} \hlkwc{type}\hlstd{=}\hlstr{"joint"}\hlstd{)}
\hlkwd{ftable}\hlstd{(tt)}
\end{alltt}
\begin{verbatim}
##         c    yes     no
## a   b                  
## yes yes    1e+00 1e-200
##     no     0e+00 1e-200
## no  yes    0e+00  0e+00
##     no     0e+00 1e-200
\end{verbatim}
\begin{alltt}
\hlkwd{querygrain}\hlstd{(}\hlkwd{setEvidence}\hlstd{(bn,} \hlkwc{evidence}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{a}\hlstd{=}\hlstr{"no"}\hlstd{,} \hlkwc{c}\hlstd{=}\hlstr{"yes"}\hlstd{)))}
\end{alltt}
\begin{verbatim}
## $b
## b
## yes  no 
## NaN NaN
\end{verbatim}
\end{kframe}
\end{knitrout}


\subsection{Brute force computations and why they fail}
\label{sec:brute-force-comp}


The \grbn\ package makes computations as those outlined above in a
very efficient way; please see the references.  However, it is in this
small example also possible to make the computations directly: We can
construct the joint distribution (an array with $2^8=256$ entries) directly as:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{joint} \hlkwb{<-} \hlkwd{ar_prod_list}\hlstd{(chest_cpt)}
\hlkwd{dim}\hlstd{(joint)}
\end{alltt}
\begin{verbatim}
## [1] 2 2 2 2 2 2 2 2
\end{verbatim}
\begin{alltt}
\hlstd{joint}  \hlopt{%>%} \hlstd{as.data.frame.table} \hlopt{%>%} \hlstd{head}
\end{alltt}
\begin{verbatim}
##   dysp bronc either lung tub asia smoke xray      Freq
## 1  yes   yes    yes  yes yes  yes   yes  yes 1.323e-05
## 2   no   yes    yes  yes yes  yes   yes  yes 1.470e-06
## 3  yes    no    yes  yes yes  yes   yes  yes 6.860e-06
## 4   no    no    yes  yes yes  yes   yes  yes 2.940e-06
## 5  yes   yes     no  yes yes  yes   yes  yes 0.000e+00
## 6   no   yes     no  yes yes  yes   yes  yes 0.000e+00
\end{verbatim}
\end{kframe}
\end{knitrout}

This will clearly fail even moderate size problems: For example, a
model with $80$
nodes each with $10$
levels will give a joint state space with $10^{80}$
states; that is about the number of atoms in the universe. Similarly,
$265$
binary variables will result in a joint state space of about the same
size. Yet, \grbn\ has been used succesfully in models with tens of
thousand variables.  The ``trick'' in \grbn\ is to make all
computations without ever forming the joint distribution. 

However, we
can do all the computations by brute force methods as we will
illustrate here:

Marginal distributions are
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{ar_marg}\hlstd{(joint,} \hlstr{"lung"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## lung
##   yes    no 
## 0.055 0.945
\end{verbatim}
\begin{alltt}
\hlkwd{ar_marg}\hlstd{(joint,} \hlstr{"bronc"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## bronc
##  yes   no 
## 0.45 0.55
\end{verbatim}
\end{kframe}
\end{knitrout}

Conditioning on evidence can be done in different ways: The conditional density is a $6$--way slice of the original $8$--way joint distribution:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{ev} \hlkwb{<-} \hlkwd{list}\hlstd{(}\hlkwc{asia}\hlstd{=}\hlstr{"yes"}\hlstd{,} \hlkwc{dysp}\hlstd{=}\hlstr{"yes"}\hlstd{)}
\hlstd{cond1} \hlkwb{<-} \hlkwd{ar_slice}\hlstd{(joint,} \hlkwc{slice}\hlstd{=ev)}
\hlstd{cond1} \hlkwb{<-} \hlstd{cond1} \hlopt{/} \hlkwd{sum}\hlstd{(cond1)}
\hlkwd{dim}\hlstd{(cond1)}
\end{alltt}
\begin{verbatim}
## [1] 2 2 2 2 2 2
\end{verbatim}
\begin{alltt}
\hlkwd{ar_marg}\hlstd{(cond1,} \hlstr{"lung"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## lung
##        yes         no 
## 0.09952515 0.90047485
\end{verbatim}
\begin{alltt}
\hlkwd{ar_marg}\hlstd{(cond1,} \hlstr{"bronc"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## bronc
##       yes        no 
## 0.8114021 0.1885979
\end{verbatim}
\end{kframe}
\end{knitrout}

Alternatively, multiply all entries not consistent by zero and all other entries by one and then marginalize:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{cond2} \hlkwb{<-} \hlkwd{ar_slice_mult}\hlstd{(joint,} \hlkwc{slice}\hlstd{=ev)}
\hlstd{cond2} \hlkwb{<-} \hlstd{cond2} \hlopt{/} \hlkwd{sum}\hlstd{(cond2)}
\hlkwd{dim}\hlstd{(cond2)}
\end{alltt}
\begin{verbatim}
## [1] 2 2 2 2 2 2 2 2
\end{verbatim}
\begin{alltt}
\hlkwd{ar_marg}\hlstd{(cond2,} \hlstr{"lung"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## lung
##        yes         no 
## 0.09952515 0.90047485
\end{verbatim}
\begin{alltt}
\hlkwd{ar_marg}\hlstd{(cond2,} \hlstr{"bronc"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## bronc
##       yes        no 
## 0.8114021 0.1885979
\end{verbatim}
\end{kframe}
\end{knitrout}



% << >>= 
% f1 <- function(){
%     cond1 <- ar_slice(joint, slice=ev)
%     cond1 <- cond1 / sum(cond1)
%     dim(cond1)
%     ar_marg(cond1, "lung")    
% }
% f2 <- function(){
%     cond2 <- ar_slice_mult(joint, slice=ev)
%     cond2 <- cond2 / sum(cond2)
%     dim(cond2)
%     ar_marg(cond2, "lung")    
% }
% f1()
% f2()
% querygrain(chest_bn, nodes="lung", evidence=ev)
% chest_bn1 <- propagate(chest_bn)
% if (require(microbenchmark)){
%     microbenchmark(f1(), f2(),
%                    f3=querygrain(chest_bn, nodes="lung", evidence=ev),
%                    f4=querygrain(chest_bn, nodes="lung", evidence=ev)                   
%                    )                   
% }
% @

\section{Hard  and virtual (likelihood) evidence}
\label{sec:hard-virt-likel}

Below we describe  how to work with virtual evidence (also known
as likelihood evidence) in \grbn. This is done via the function
\code{setEvidence()}.

The clique potential representation in a Bayesian network gives
\begin{displaymath}
  p(x) \propto \psi(x) = \prod_{C} \psi_C(x_C)
\end{displaymath}
where we recall that the whole idea in computations with Bayesian
networks is to avoid calculation the product on the right hand
side. Instead computations are based on propagation (multiplying,
dividing and summing clique potentials $\psi_C$ in an appropriate
order, and such an appropriate order comes from a junction tree).
The normalizing constant, say $c=\sum_x \psi(x)$, comes out of
propagation as a ``by product''.

Suppose a set of nodes $E$ are known to have a specific value,
i.e. $x_E=x^*_E$. This is called hard evidence. The probability of
the event $x_E=x^*_E$ is
\begin{displaymath}
  p(x_E=x^*_E)=E_p\{I(x_E=x^*_E)\} = \sum_x I(x_E=x^*_E) p(x)
  = \frac{1}{c} \sum_x I(x_E=x^*_E) \psi(x)
\end{displaymath}

The computations are based on modifying the clique potentials $\psi_C$
by giving value zero to states in $\psi_C$ which are not consistent
with $x_E=x^*_E$. This can be achieved with an indicator function, say
$L_C(x_C)$ such that we obtain a set of new potentials $\tilde \psi_C
= L_C(x_C) \psi_C(x_C)$. Propagation with these new potentials gives,
as a by product, $\tilde c=\sum \tilde \psi(x)$ where
$\tilde\psi(x)= \prod_C \tilde\psi_C(x_C)$. Consequently, we have
$p(x_E=x^*_E)=\tilde c / c$.

In a more general setting we may have non--negative weights $L(x)$ for
each value of $x$. We may calculate
\begin{displaymath}
  E_p\{L(X)\} = \sum_x L(x)p(x)
\end{displaymath}
If $L(X)$ factorizes as $L(X)=L_C(X_C)$ then the computations are
carried out as outlined above, i.e.\ by the message passing scheme.


\subsection{An excerpt of the chest clinic network}
\label{sec:an-excerpt-chest}


Consider the following excerpt of
the chest clinic network which is described in the paper mentioned
above.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{yn} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"yes"}\hlstd{,}\hlstr{"no"}\hlstd{)}
\hlstd{a}    \hlkwb{<-} \hlkwd{cptable}\hlstd{(}\hlopt{~}\hlstd{asia,} \hlkwc{values}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{99}\hlstd{),}\hlkwc{levels}\hlstd{=yn)}
\hlstd{t.a}  \hlkwb{<-} \hlkwd{cptable}\hlstd{(}\hlopt{~}\hlstd{tub}\hlopt{|}\hlstd{asia,} \hlkwc{values}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{5}\hlstd{,}\hlnum{95}\hlstd{,}\hlnum{1}\hlstd{,}\hlnum{99}\hlstd{),}\hlkwc{levels}\hlstd{=yn)}

\hlstd{(plist1} \hlkwb{<-} \hlkwd{compileCPT}\hlstd{(}\hlkwd{list}\hlstd{(a, t.a)))}
\end{alltt}
\begin{verbatim}
## cpt_spec with probabilities:
##  P( asia )
##  P( tub | asia )
\end{verbatim}
\begin{alltt}
\hlstd{plist1[[}\hlnum{1}\hlstd{]]}
\end{alltt}
\begin{verbatim}
## asia
##  yes   no 
## 0.01 0.99
\end{verbatim}
\begin{alltt}
\hlstd{plist1[[}\hlnum{2}\hlstd{]]}
\end{alltt}
\begin{verbatim}
##      asia
## tub    yes   no
##   yes 0.05 0.01
##   no  0.95 0.99
\end{verbatim}
\begin{alltt}
\hlstd{(chest1} \hlkwb{<-} \hlkwd{grain}\hlstd{(plist1))}
\end{alltt}
\begin{verbatim}
## Independence network: Compiled: FALSE Propagated: FALSE 
##   Nodes: chr [1:2] "asia" "tub"
\end{verbatim}
\begin{alltt}
\hlkwd{querygrain}\hlstd{(chest1)}
\end{alltt}
\begin{verbatim}
## $asia
## asia
##  yes   no 
## 0.01 0.99 
## 
## $tub
## tub
##    yes     no 
## 0.0104 0.9896
\end{verbatim}
\end{kframe}
\end{knitrout}

\subsection{Specifying hard evidence}
\label{sec:hard-evidence}

Suppose we want to make a diagnosis about tuberculosis given the
evidence that a person has recently been to Asia. The functions
\code{setFinding()} (which has been in \grbn\ for years) and
\code{setEvidence()} (which is a recent addition to \grbn) can both be used for this
purpose. The following forms are equivalent (\verb|setFinding()| is kept in \grbn\ for backward compatibility):

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{setEvidence}\hlstd{(chest1,} \hlkwc{evidence}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{asia}\hlstd{=}\hlstr{"yes"}\hlstd{))}
\end{alltt}
\begin{verbatim}
## Independence network: Compiled: TRUE Propagated: TRUE 
##   Nodes: chr [1:2] "asia" "tub"
##   Evidence:
##   nodes is.hard.evidence hard.state
## 1  asia             TRUE        yes
##   pEvidence: 0.010000
\end{verbatim}
\begin{alltt}
\hlkwd{setEvidence}\hlstd{(chest1,} \hlkwc{nodes}\hlstd{=}\hlstr{"asia"}\hlstd{,} \hlkwc{states}\hlstd{=}\hlstr{"yes"}\hlstd{)}
\end{alltt}
\begin{verbatim}
## Independence network: Compiled: TRUE Propagated: TRUE 
##   Nodes: chr [1:2] "asia" "tub"
##   Evidence:
##   nodes is.hard.evidence hard.state
## 1  asia             TRUE        yes
##   pEvidence: 0.010000
\end{verbatim}
\begin{alltt}
\hlcom{## setFinding(chest1, nodes="asia", states="yes")}
\end{alltt}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{querygrain}\hlstd{(}\hlkwd{setEvidence}\hlstd{(chest1,} \hlkwc{evidence}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{asia}\hlstd{=}\hlstr{"yes"}\hlstd{)))}
\end{alltt}
\begin{verbatim}
## $tub
## tub
##  yes   no 
## 0.05 0.95
\end{verbatim}
\end{kframe}
\end{knitrout}

\subsection{What is virtual evidence (also called likelihood evidence)
?}
\label{sec:virt-evid-likel}

Suppose we do not know with certainty whether a patient has
recently been to Asia (perhaps the patient is too ill to
tell). However the patient (if he/she is Caucasian) may be unusually
tanned and this lends support to the hypothesis of a recent visit to
Asia.

To accommodate we create an extended network with an extra
node for which we enter evidence.  However, it is NOT necessary to do
so in practice, because we may equivalently enter the virtual evidence
in the original network.

We can then introduce a new variable
\code{guess.asia} with \code{asia} as its only parent.\footnote{FIXME: Hvorfor vil parray ikke gaa vaek...}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{g.a} \hlkwb{<-} \hlkwd{parray}\hlstd{(}\hlkwd{c}\hlstd{(}\hlstr{"guess.asia"}\hlstd{,} \hlstr{"asia"}\hlstd{),} \hlkwc{levels}\hlstd{=}\hlkwd{list}\hlstd{(yn, yn),}
              \hlkwc{values}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{.8}\hlstd{,}\hlnum{.2}\hlstd{,} \hlnum{.1}\hlstd{,}\hlnum{.9}\hlstd{))}
\end{alltt}
\end{kframe}
\end{knitrout}

This reflects the assumption that for patients who have recently been
to Asia we would guess so in 80\% of the cases, whereas for patients who have
not recently been to A we would (erroneously) guess that they have
recently been to Asia in 10\% of the cases.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{(plist2} \hlkwb{<-} \hlkwd{compileCPT}\hlstd{(}\hlkwd{list}\hlstd{(a, t.a, g.a )))}
\end{alltt}
\begin{verbatim}
## cpt_spec with probabilities:
##  P( asia )
##  P( tub | asia )
##  P( guess.asia | asia )
\end{verbatim}
\begin{alltt}
\hlstd{(chest2} \hlkwb{<-} \hlkwd{grain}\hlstd{(plist2))}
\end{alltt}
\begin{verbatim}
## Independence network: Compiled: FALSE Propagated: FALSE 
##   Nodes: chr [1:3] "asia" "tub" "guess.asia"
\end{verbatim}
\begin{alltt}
\hlkwd{querygrain}\hlstd{( chest2 )}
\end{alltt}
\begin{verbatim}
## $asia
## asia
##  yes   no 
## 0.01 0.99 
## 
## $tub
## tub
##    yes     no 
## 0.0104 0.9896 
## 
## $guess.asia
## guess.asia
##   yes    no 
## 0.107 0.893
\end{verbatim}
\end{kframe}
\end{knitrout}


Now specify the guess or judgment, that the person has recently been
to Asia:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{querygrain}\hlstd{(}\hlkwd{setEvidence}\hlstd{(chest2,} \hlkwc{evidence}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{guess.asia}\hlstd{=}\hlstr{"yes"}\hlstd{)))}
\end{alltt}
\begin{verbatim}
## $asia
## asia
##        yes         no 
## 0.07476636 0.92523364 
## 
## $tub
## tub
##        yes         no 
## 0.01299065 0.98700935
\end{verbatim}
\end{kframe}
\end{knitrout}

\subsection{Specifying virtual evidence}
\label{sec:spec-virt-evid}

The same guess or judgment can be specified as virtual evidence
(also called likelihood evidence) for the original network:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{querygrain}\hlstd{(}\hlkwd{setEvidence}\hlstd{(chest1,} \hlkwc{evidence}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{asia}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{.8}\hlstd{,} \hlnum{.1}\hlstd{))))}
\end{alltt}
\begin{verbatim}
## $tub
## tub
##        yes         no 
## 0.01299065 0.98700935
\end{verbatim}
\end{kframe}
\end{knitrout}

This also means that specifying that specifying \code{asia='yes'} can
be done as
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{querygrain}\hlstd{(}\hlkwd{setEvidence}\hlstd{(chest1,} \hlkwc{evidence}\hlstd{=}\hlkwd{list}\hlstd{(}\hlkwc{asia}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{0}\hlstd{))))}
\end{alltt}
\begin{verbatim}
## $tub
## tub
##  yes   no 
## 0.05 0.95
\end{verbatim}
\end{kframe}
\end{knitrout}


\subsection{A mixture of a discrete and a continuous variable}
\label{sec:ixture}

\grbn\ only handles discrete variables with a finite state space, but
using likelihood evidence it is possible to work with networks with
both discrete and continuous variables (or other types of variables).
This is possible only when he networks
have a specific structure. This is possible when no discrete variable
has non--discrete parents.\footnote{SH: Expand this.}

Take a simple example: $x$ is a discrete variable with levels $1$ and
$2$; $y_1|x=k \sim N(\mu_k, \sigma^2_k)$ and $y_2|x=k \sim
Poi(\lambda_k)$ where $k=1,2$. The joint distribution is
\begin{displaymath}
  p(x,y_1, y_2) = p(x)p(y_1|x)p(y_2|x)
\end{displaymath}

Suppose the interest is in the distribution of $x$ given
$y_1=y_1^*$ and $y_2=y_2^*$. We then have
\begin{displaymath}
  p(x|y_1^*, y_2^*) \propto p(x) p(y_1^*|x)p(y_2^*|x) =
  p(x) L_1(x) L_2(x)
\end{displaymath}







\section{Building networks from data}
\label{sec:using-textttsm-argum}

The following two graphs specify the same model:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{dG}  \hlkwb{<-} \hlkwd{dag}\hlstd{(}\hlopt{~}\hlstd{A}\hlopt{:}\hlstd{B} \hlopt{+} \hlstd{B}\hlopt{:}\hlstd{C)}
\hlstd{uG}  \hlkwb{<-} \hlkwd{ug}\hlstd{(}\hlopt{~}\hlstd{A}\hlopt{:}\hlstd{B} \hlopt{+} \hlstd{B}\hlopt{:}\hlstd{C)}
\hlkwd{par}\hlstd{(}\hlkwc{mfrow}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,}\hlnum{2}\hlstd{));} \hlkwd{plot}\hlstd{( dG );} \hlkwd{plot}\hlstd{( uG )}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figures/grain-unnamed-chunk-34-1} 

\end{knitrout}

Suppose data is
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{dat} \hlkwb{<-} \hlkwd{ar_new}\hlstd{(}\hlkwd{c}\hlstd{(}\hlstr{"A"}\hlstd{,} \hlstr{"B"}\hlstd{,} \hlstr{"C"}\hlstd{),} \hlkwc{levels}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{2}\hlstd{,} \hlnum{2}\hlstd{,} \hlnum{2}\hlstd{),} \hlkwc{values}\hlstd{=}\hlkwd{c}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{0}\hlstd{,} \hlnum{2}\hlstd{,} \hlnum{3}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{2}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{4}\hlstd{))}
\hlkwd{class}\hlstd{(dat)}
\end{alltt}
\begin{verbatim}
## [1] "array"
\end{verbatim}
\end{kframe}
\end{knitrout}

A network can be built from data using:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{gr.dG} \hlkwb{<-} \hlkwd{compile}\hlstd{(} \hlkwd{grain}\hlstd{( dG,} \hlkwc{data}\hlstd{=dat ) )}
\hlstd{gr.uG} \hlkwb{<-} \hlkwd{compile}\hlstd{(} \hlkwd{grain}\hlstd{( uG,} \hlkwc{data}\hlstd{=dat ) )}
\end{alltt}
\end{kframe}
\end{knitrout}

However, when there are zeros in the table, care must be taken.

\subsection{Extracting information from tables}
\label{sec:extr-inform-from}

In the process of creating networks, conditional probability tables
are extracted when the graph is a dag and clique potentials are
extracted when the graph is a chordal (i.e.\ triangulated) undirected
graph. This takes place as follows (internally):

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{extractCPT}\hlstd{(dat, dG)}
\end{alltt}
\begin{verbatim}
## $A
##     B
## A           B1  B2
##   A1 0.3333333 0.3
##   A2 0.6666667 0.7
## 
## $B
##     C
## B    C1    C2
##   B1  0 0.375
##   B2  1 0.625
## 
## $C
## C
##        C1        C2 
## 0.3846154 0.6153846 
## 
## attr(,"graph")
## A graphNEL graph with directed edges
## Number of Nodes = 3 
## Number of Edges = 2 
## attr(,"class")
## [1] "cpt_rep"
\end{verbatim}
\begin{alltt}
\hlkwd{c}\hlstd{(}\hlkwd{extractPOT}\hlstd{(dat, uG ))}
\end{alltt}
\begin{verbatim}
## [[1]]
##     A
## B            A1        A2
##   B1 0.07692308 0.1538462
##   B2 0.23076923 0.5384615
## 
## [[2]]
##     C
## B     C1  C2
##   B1 0.0 1.0
##   B2 0.5 0.5
\end{verbatim}
\end{kframe}
\end{knitrout}

The conditional probability table $P(A|B)$ contains \code{NaN}s
because
\begin{displaymath}
  P(A|B=B1)=\frac{n(A,B=B1)}{\sum_A n(A,B=B1)} = \frac{0}{0} = \mbox{NaN}
\end{displaymath}

For this reason the network \code{gr.dG} above will fail to compile
whereas \code{gr.uG} will work, but it may not give the expected results.

\subsection{Using smooth}
\label{sec:using-smooth}

To illustrate what goes on, we can extract the distributions from data
as follows:\footnote{SH: FIXME Use new functions from gRbase.}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{p.A.g.B} \hlkwb{<-} \hlkwd{tableDiv}\hlstd{(dat,} \hlkwd{tableMargin}\hlstd{(dat,} \hlstr{"B"}\hlstd{))}
\hlstd{p.B}     \hlkwb{<-} \hlkwd{tableMargin}\hlstd{(dat,} \hlstr{"B"}\hlstd{)} \hlopt{/} \hlkwd{sum}\hlstd{(dat)}
\hlstd{p.AB}    \hlkwb{<-} \hlkwd{tableMult}\hlstd{( p.A.g.B, p.B)}
\end{alltt}
\end{kframe}
\end{knitrout}

However, the result is slightly misleading because \code{tableDiv}
sets $0/0=0$.

In \grain\ there is a \code{smooth} argument that will add a small
number to the cell entries before extracting tables, i.e.
\begin{displaymath}
  P(A|B=B1)=\frac{n(A,B=B1)+\epsilon}{\sum_A ( n(A,B=B1) + \epsilon) }
  = \frac{\epsilon}{2\epsilon} = 0.5
\end{displaymath}
and
\begin{displaymath}
  P(B)= \frac{\sum_A (n(A,B)+\epsilon)}{\sum_{AB} (n(A,B)+\epsilon)}
\end{displaymath}

We can mimic this as follows:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{e} \hlkwb{<-} \hlnum{1e-2}
\hlstd{(dat.e} \hlkwb{<-} \hlstd{dat} \hlopt{+} \hlstd{e)}
\end{alltt}
\begin{verbatim}
## , , C = C1
## 
##     B
## A      B1   B2
##   A1 0.01 2.01
##   A2 0.01 3.01
## 
## , , C = C2
## 
##     B
## A      B1   B2
##   A1 1.01 1.01
##   A2 2.01 4.01
\end{verbatim}
\end{kframe}
\end{knitrout}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{pe.A.g.B} \hlkwb{<-} \hlkwd{tableDiv}\hlstd{(dat.e,} \hlkwd{tableMargin}\hlstd{(dat,} \hlstr{"B"}\hlstd{))}
\hlstd{pe.B} \hlkwb{<-} \hlkwd{tableMargin}\hlstd{(dat.e,} \hlstr{"B"}\hlstd{)}\hlopt{/}\hlkwd{sum}\hlstd{(dat.e)}
\hlstd{pe.AB}  \hlkwb{<-} \hlkwd{tableMult}\hlstd{( pe.A.g.B, pe.B )}
\end{alltt}
\end{kframe}
\end{knitrout}

However this resulting joint distribution is different from what is
obtained from the adjusted table itself
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{dat.e} \hlopt{/} \hlkwd{sum}\hlstd{(dat.e)}
\end{alltt}
\begin{verbatim}
## , , C = C1
## 
##     B
## A             B1        B2
##   A1 0.000764526 0.1536697
##   A2 0.000764526 0.2301223
## 
## , , C = C2
## 
##     B
## A            B1         B2
##   A1 0.07721713 0.07721713
##   A2 0.15366972 0.30657492
\end{verbatim}
\end{kframe}
\end{knitrout}

This difference appears in the \grbn\ networks.

\subsection{Extracting tables}
\label{sec:extracting-tables}

One can do
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{gr.dG} \hlkwb{<-} \hlkwd{compile}\hlstd{(}\hlkwd{grain}\hlstd{(dG,} \hlkwc{data}\hlstd{=dat,} \hlkwc{smooth}\hlstd{=e))}
\end{alltt}
\end{kframe}
\end{knitrout}

which (internally) corresponds to
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{extractCPT}\hlstd{(dat, dG,} \hlkwc{smooth}\hlstd{=e)}
\end{alltt}
\begin{verbatim}
## $A
##     B
## A           B1        B2
##   A1 0.3344371 0.3003992
##   A2 0.6655629 0.6996008
## 
## $B
##     C
## B             C1        C2
##   B1 0.001992032 0.3753117
##   B2 0.998007968 0.6246883
## 
## $C
## C
##        C1        C2 
## 0.3847926 0.6152074 
## 
## attr(,"graph")
## A graphNEL graph with directed edges
## Number of Nodes = 3 
## Number of Edges = 2 
## attr(,"class")
## [1] "cpt_rep"
\end{verbatim}
\end{kframe}
\end{knitrout}

We get
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{querygrain}\hlstd{(gr.dG)}
\end{alltt}
\begin{verbatim}
## $A
## A
##        A1        A2 
## 0.3082845 0.6917155 
## 
## $B
## B
##        B1        B2 
## 0.2316611 0.7683389 
## 
## $C
## C
##        C1        C2 
## 0.3847926 0.6152074
\end{verbatim}
\begin{alltt}
\hlkwd{querygrain}\hlstd{(gr.uG)}
\end{alltt}
\begin{verbatim}
## $B
## B
##        B1        B2 
## 0.2307692 0.7692308 
## 
## $A
## A
##        A1        A2 
## 0.3076923 0.6923077 
## 
## $C
## C
##        C1        C2 
## 0.3846154 0.6153846
\end{verbatim}
\end{kframe}
\end{knitrout}

However, if we condition on \code{B=B1} we get:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{querygrain}\hlstd{(}\hlkwd{setFinding}\hlstd{(gr.dG,} \hlkwc{nodes}\hlstd{=}\hlstr{"B"}\hlstd{,} \hlkwc{states}\hlstd{=}\hlstr{"B1"}\hlstd{))}
\end{alltt}
\begin{verbatim}
## $A
## A
##        A1        A2 
## 0.3344371 0.6655629 
## 
## $C
## C
##          C1          C2 
## 0.003308796 0.996691204
\end{verbatim}
\begin{alltt}
\hlkwd{querygrain}\hlstd{(}\hlkwd{setFinding}\hlstd{(gr.uG,} \hlkwc{nodes}\hlstd{=}\hlstr{"B"}\hlstd{,} \hlkwc{states}\hlstd{=}\hlstr{"B1"}\hlstd{))}
\end{alltt}
\begin{verbatim}
## $A
## A
##        A1        A2 
## 0.3333333 0.6666667 
## 
## $C
## C
## C1 C2 
##  0  1
\end{verbatim}
\end{kframe}
\end{knitrout}

so the ``problem'' with zero entries shows up in a different
place. However, the answer is not necessarily wrong; the answer simply
states that $P(A|B=B1)$ is undefined.
To ``remedy'' we can use the \code{smooth} argument:
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{gr.uG} \hlkwb{<-} \hlkwd{compile}\hlstd{(}\hlkwd{grain}\hlstd{(uG,} \hlkwc{data}\hlstd{=dat,} \hlkwc{smooth}\hlstd{=e))}
\end{alltt}
\end{kframe}
\end{knitrout}
which (internally) corresponds to
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{c}\hlstd{(}\hlkwd{extractPOT}\hlstd{(dat, uG,} \hlkwc{smooth}\hlstd{=e))}
\end{alltt}
\begin{verbatim}
## [[1]]
##     A
## B            A1        A2
##   B1 0.07745399 0.1541411
##   B2 0.23082822 0.5375767
## 
## [[2]]
##     C
## B             C1        C2
##   B1 0.003311258 0.9966887
##   B2 0.500000000 0.5000000
\end{verbatim}
\end{kframe}
\end{knitrout}

Notice that the results are not exactly identical:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{querygrain}\hlstd{(gr.uG)}
\end{alltt}
\begin{verbatim}
## $B
## B
##        B1        B2 
## 0.2315951 0.7684049 
## 
## $A
## A
##        A1        A2 
## 0.3082822 0.6917178 
## 
## $C
## C
##        C1        C2 
## 0.3849693 0.6150307
\end{verbatim}
\begin{alltt}
\hlkwd{querygrain}\hlstd{(gr.dG)}
\end{alltt}
\begin{verbatim}
## $A
## A
##        A1        A2 
## 0.3082845 0.6917155 
## 
## $B
## B
##        B1        B2 
## 0.2316611 0.7683389 
## 
## $C
## C
##        C1        C2 
## 0.3847926 0.6152074
\end{verbatim}
\end{kframe}
\end{knitrout}


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{querygrain}\hlstd{(}\hlkwd{setFinding}\hlstd{(gr.uG,} \hlkwc{nodes}\hlstd{=}\hlstr{"B"}\hlstd{,} \hlkwc{states}\hlstd{=}\hlstr{"B1"}\hlstd{))}
\end{alltt}
\begin{verbatim}
## $A
## A
##        A1        A2 
## 0.3344371 0.6655629 
## 
## $C
## C
##          C1          C2 
## 0.003311258 0.996688742
\end{verbatim}
\begin{alltt}
\hlkwd{querygrain}\hlstd{(}\hlkwd{setFinding}\hlstd{(gr.dG,} \hlkwc{nodes}\hlstd{=}\hlstr{"B"}\hlstd{,} \hlkwc{states}\hlstd{=}\hlstr{"B1"}\hlstd{))}
\end{alltt}
\begin{verbatim}
## $A
## A
##        A1        A2 
## 0.3344371 0.6655629 
## 
## $C
## C
##          C1          C2 
## 0.003308796 0.996691204
\end{verbatim}
\end{kframe}
\end{knitrout}


\bibliography{grain}

\end{document}


%%\SweaveInput{Rmarkup.STY}
%% ------------------------
%% \definecolor{darkred}{rgb}{.7,0,0}
%% \definecolor{midnightblue}{rgb}{0.098,0.098,0.439}
%% 
%% \DefineVerbatimEnvironment{Sinput}{Verbatim}{
%%   fontfamily=tt,
%%   %%fontseries=b,
%%   %% xleftmargin=2em,
%%   formatcom={\color{midnightblue}}
%% }
%% \DefineVerbatimEnvironment{Soutput}{Verbatim}{
%%   fontfamily=tt,
%%   %%fontseries=b,
%%   %% xleftmargin=2em,
%%   formatcom={\color{darkred}}
%% }
%% \DefineVerbatimEnvironment{Scode}{Verbatim}{
%%   fontfamily=tt,
%%   %%fontseries=b,
%%   %% xleftmargin=2em,
%%   formatcom={\color{blue}}
%% }
%% 
%% \fvset{listparameters={\setlength{\topsep}{-2pt}}}
%% \renewenvironment{Schunk}{\linespread{.90}}{}
%% %% ------------------------
%% 




% We can look closer into this zero--probability issue. Because the node
% \code{either} is logical, half of the configurations will have zero probability:

% <<>>=
% tt <- querygrain(chest_bn, type="joint")
% sum(tt == 0) / length(tt)
% @ %def

% In particular the configuration above has zero probability
% <<>>=
% sum(ar_slice(tt, list(either="no", tub="yes")))
% @ %def
